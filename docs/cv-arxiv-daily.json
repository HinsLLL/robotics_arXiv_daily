{"Manipulation": {"2504.02792": "|**2025-04-03**|**Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets**|Abhishek Gupta Team|[2504.02792](http://arxiv.org/abs/2504.02792)|null|\n", "2504.02477": "|**2025-04-03**|**Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision**|Shibiao Xu Team|[2504.02477](http://arxiv.org/abs/2504.02477)|null|\n", "2504.02069": "|**2025-04-02**|**RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics**|Qiang Nie Team|[2504.02069](http://arxiv.org/abs/2504.02069)|null|\n", "2504.01959": "|**2025-04-02**|**Slot-Level Robotic Placement via Visual Imitation from Single Human Video**|Arsalan Mousavian Team|[2504.01959](http://arxiv.org/abs/2504.01959)|null|\n", "2504.01766": "|**2025-04-02**|**Learning with Imperfect Models: When Multi-step Prediction Mitigates Compounding Error**|Nikolai Matni Team|[2504.01766](http://arxiv.org/abs/2504.01766)|null|\n", "2504.01708": "|**2025-04-02**|**TransforMerger: Transformer-based Voice-Gesture Fusion for Robust Human-Robot Communication**|Karla Stepanova Team|[2504.01708](http://arxiv.org/abs/2504.01708)|null|\n", "2504.01554": "|**2025-04-02**|**8-DoFs Cable Driven Parallel Robots for Bimanual Teleportation**|Josie Hughes Team|[2504.01554](http://arxiv.org/abs/2504.01554)|null|\n", "2504.01301": "|**2025-04-02**|**Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers**|Yuki Uranishi Team|[2504.01301](http://arxiv.org/abs/2504.01301)|null|\n", "2504.01260": "|**2025-04-02**|**The Social Life of Industrial Arms: How Arousal and Attention Shape Human-Robot Interaction**|Matthew K. X. J Pan Team|[2504.01260](http://arxiv.org/abs/2504.01260)|null|\n", "2504.00707": "|**2025-04-01**|**Energy Weighted Learning Progress Guided Interleaved Multi-Task Learning**|Erhan Oztop Team|[2504.00707](http://arxiv.org/abs/2504.00707)|null|\n", "2504.00614": "|**2025-04-01**|**Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs**|Masaya Kinoshita Team|[2504.00614](http://arxiv.org/abs/2504.00614)|null|\n", "2504.00420": "|**2025-04-01**|**Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation**|Dong Wang Team|[2504.00420](http://arxiv.org/abs/2504.00420)|null|\n", "2504.00234": "|**2025-03-31**|**CBIL: Collective Behavior Imitation Learning for Fish from Real Videos**|Taku Komura Team|[2504.00234](http://arxiv.org/abs/2504.00234)|null|\n", "2503.24361": "|**2025-04-02**|**Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation**|Yuke Zhu Team|[2503.24361](http://arxiv.org/abs/2503.24361)|null|\n", "2503.24278": "|**2025-04-02**|**AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World**|Sergey Levine Team|[2503.24278](http://arxiv.org/abs/2503.24278)|**[link](https://github.com/zhouzypaul/auto_eval)**|\n", "2503.24070": "|**2025-03-31**|**HACTS: a Human-As-Copilot Teleoperation System for Robot Learning**|Jian Tang Team|[2503.24070](http://arxiv.org/abs/2503.24070)|null|\n", "2503.24009": "|**2025-03-31**|**Learning 3D-Gaussian Simulators from RGB Videos**|Georg Martius Team|[2503.24009](http://arxiv.org/abs/2503.24009)|null|\n", "2503.23877": "|**2025-03-31**|**ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos**|Dinesh Jayaraman Team|[2503.23877](http://arxiv.org/abs/2503.23877)|**[link](https://github.com/everloom-129/rekep)**|\n", "2503.23835": "|**2025-03-31**|**Disambiguate Gripper State in Grasp-Based Tasks: Pseudo-Tactile as Feedback Enables Pure Simulation Learning**|Yue Wang Team|[2503.23835](http://arxiv.org/abs/2503.23835)|null|\n", "2503.23571": "|**2025-03-30**|**Can Visuo-motor Policies Benefit from Random Exploration Data? A Case Study on Stacking**|Florian T. Pokorny Team|[2503.23571](http://arxiv.org/abs/2503.23571)|null|\n"}, "VLM": {"2504.02823": "|**2025-04-03**|**STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection**|Naoufel Werghi Team|[2504.02823](http://arxiv.org/abs/2504.02823)|null|\n", "2504.02821": "|**2025-04-03**|**Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models**|Zeynep Akata Team|[2504.02821](http://arxiv.org/abs/2504.02821)|null|\n", "2504.02799": "|**2025-04-03**|**Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence**|Serena Yeung-Levy Team|[2504.02799](http://arxiv.org/abs/2504.02799)|null|\n", "2504.02765": "|**2025-04-03**|**Robot-Led Vision Language Model Wellbeing Assessment of Children**|Hatice Gunes Team|[2504.02765](http://arxiv.org/abs/2504.02765)|null|\n", "2504.02587": "|**2025-04-03**|**Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme**|Pengfei Liu Team|[2504.02587](http://arxiv.org/abs/2504.02587)|null|\n", "2504.02477": "|**2025-04-03**|**Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision**|Shibiao Xu Team|[2504.02477](http://arxiv.org/abs/2504.02477)|null|\n", "2504.02438": "|**2025-04-03**|**Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation**|Rui Yan Team|[2504.02438](http://arxiv.org/abs/2504.02438)|null|\n", "2504.02357": "|**2025-04-03**|**ReuseDroid: A VLM-empowered Android UI Test Migrator Boosted by Active Feedback**|Hailong Wang Team|[2504.02357](http://arxiv.org/abs/2504.02357)|null|\n", "2504.02349": "|**2025-04-03**|**Large (Vision) Language Models are Unsupervised In-Context Learners**|Maria Brbic Team|[2504.02349](http://arxiv.org/abs/2504.02349)|**[link](https://github.com/mlbio-epfl/joint-inference)**|\n", "2504.02259": "|**2025-04-03**|**Re-thinking Temporal Search for Long-Form Video Understanding**|Manling Li Team|[2504.02259](http://arxiv.org/abs/2504.02259)|null|\n", "2504.02244": "|**2025-04-03**|**SocialGesture: Delving into Multi-person Gesture Understanding**|James M. Rehg Team|[2504.02244](http://arxiv.org/abs/2504.02244)|null|\n", "2504.01916": "|**2025-04-02**|**FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs**|Fatima Albreiki Team|[2504.01916](http://arxiv.org/abs/2504.01916)|**[link](https://github.com/tiiuae/FineLIP)**|\n", "2504.01890": "|**2025-04-02**|**Is Temporal Prompting All We Need For Limited Labeled Action Recognition?**|Xiaobo Jin Team|[2504.01890](http://arxiv.org/abs/2504.01890)|null|\n", "2504.01838": "|**2025-04-02**|**Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by Generating Realistic Dermoscopic Images**|Abdullah-Al-Zubaer Imran Team|[2504.01838](http://arxiv.org/abs/2504.01838)|**[link](https://github.com/munia03/dermdit)**|\n", "2504.01786": "|**2025-04-02**|**BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing**|Leonidas Guibas Team|[2504.01786](http://arxiv.org/abs/2504.01786)|null|\n", "2504.01735": "|**2025-04-02**|**AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization**|Linli Xu Team|[2504.01735](http://arxiv.org/abs/2504.01735)|null|\n", "2504.01700": "|**2025-04-02**|**Reasoning LLMs for User-Aware Multimodal Conversational Agents**|Mohamed Chetouani Team|[2504.01700](http://arxiv.org/abs/2504.01700)|null|\n", "2504.01666": "|**2025-04-02**|**CLIP-SLA: Parameter-Efficient CLIP Adaptation for Continuous Sign Language Recognition**|Hamzah Luqman Team|[2504.01666](http://arxiv.org/abs/2504.01666)|**[link](https://github.com/snalyami/CLIP-SLA)**|\n", "2504.01662": "|**2025-04-02**|**BioAtt: Anatomical Prior Driven Low-Dose CT Denoising**|UiHyun Cho Team|[2504.01662](http://arxiv.org/abs/2504.01662)|null|\n", "2504.01589": "|**2025-04-02**|**Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in Vision-Language Models**|Ming-Hsuan Yang Team|[2504.01589](http://arxiv.org/abs/2504.01589)|null|\n"}, "VLA": {"2504.00907": "|**2025-04-02**|**Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning**|Roozbeh Mottaghi Team|[2504.00907](http://arxiv.org/abs/2504.00907)|null|\n", "2503.23463": "|**2025-03-30**|**OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model**|Alois C. Knoll Team|[2503.23463](http://arxiv.org/abs/2503.23463)|**[link](https://github.com/DriveVLA/OpenDriveVLA)**|\n", "2503.22020": "|**2025-03-27**|**CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models**|Tsung-Yi Lin Team|[2503.22020](http://arxiv.org/abs/2503.22020)|null|\n", "2503.20384": "|**2025-03-26**|**MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation**|Shanghang Zhang Team|[2503.20384](http://arxiv.org/abs/2503.20384)|null|\n", "2503.20020": "|**2025-03-25**|**Gemini Robotics: Bringing AI into the Physical World**|Yuxiang Zhou Team|[2503.20020](http://arxiv.org/abs/2503.20020)|null|\n", "2503.19757": "|**2025-03-25**|**Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy**|Yuntao Chen Team|[2503.19757](http://arxiv.org/abs/2503.19757)|null|\n", "2503.19516": "|**2025-03-25**|**DataPlatter: Boosting Robotic Manipulation Generalization with Minimal Costly Data**|Lin Ma Team|[2503.19516](http://arxiv.org/abs/2503.19516)|null|\n", "2503.14734": "|**2025-03-27**|**GR00T N1: An Open Foundation Model for Generalist Humanoid Robots**|Yuke Zhu Team|[2503.14734](http://arxiv.org/abs/2503.14734)|null|\n", "2503.13446": "|**2025-03-17**|**MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation**|Haibin Yan Team|[2503.13446](http://arxiv.org/abs/2503.13446)|null|\n", "2503.14526": "|**2025-03-15**|**ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis**|Mingyu Ding Team|[2503.14526](http://arxiv.org/abs/2503.14526)|null|\n", "2503.10631": "|**2025-03-17**|**HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model**|Shanghang Zhang Team|[2503.10631](http://arxiv.org/abs/2503.10631)|null|\n", "2503.09527": "|**2025-03-12**|**CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games**|Bo Zheng Team|[2503.09527](http://arxiv.org/abs/2503.09527)|null|\n", "2503.08007": "|**2025-03-11**|**MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models**|Zongyuan Ge Team|[2503.08007](http://arxiv.org/abs/2503.08007)|null|\n", "2503.07511": "|**2025-03-10**|**PointVLA: Injecting the 3D World into Vision-Language-Action Models**|Yichen Zhu Team|[2503.07511](http://arxiv.org/abs/2503.07511)|null|\n", "2503.05833": "|**2025-03-06**|**Refined Policy Distillation: From VLA Generalists to RL Experts**|Florian Walter Team|[2503.05833](http://arxiv.org/abs/2503.05833)|null|\n", "2503.04163": "|**2025-03-06**|**VLA Model-Expert Collaboration for Bi-directional Manipulation Learning**|Zeng-Guang Hou Team|[2503.04163](http://arxiv.org/abs/2503.04163)|null|\n", "2503.03734": "|**2025-03-26**|**OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction**|Pieter Abbeel Team|[2503.03734](http://arxiv.org/abs/2503.03734)|null|\n", "2503.03480": "|**2025-03-05**|**SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning**|Yaodong Yang Team|[2503.03480](http://arxiv.org/abs/2503.03480)|null|\n", "2503.02310": "|**2025-03-04**|**Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding**|Haoang Li Team|[2503.02310](http://arxiv.org/abs/2503.02310)|null|\n", "2503.01378": "|**2025-03-03**|**CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs**|Dzmitry Tsetserukou Team|[2503.01378](http://arxiv.org/abs/2503.01378)|null|\n"}, "Humanoid": {"2504.01260": "|**2025-04-02**|**The Social Life of Industrial Arms: How Arousal and Attention Shape Human-Robot Interaction**|Matthew K. X. J Pan Team|[2504.01260](http://arxiv.org/abs/2504.01260)|null|\n", "2504.01165": "|**2025-04-01**|**Extended Hybrid Zero Dynamics for Bipedal Walking of the Knee-less Robot SLIDER**|Petar Kormushev Team|[2504.01165](http://arxiv.org/abs/2504.01165)|null|\n", "2504.00614": "|**2025-04-01**|**Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs**|Masaya Kinoshita Team|[2504.00614](http://arxiv.org/abs/2504.00614)|null|\n", "2503.23601": "|**2025-03-30**|**Exploring GPT-4 for Robotic Agent Strategy with Real-Time State Feedback and a Reactive Behaviour Framework**|Ysobel Sims Team|[2503.23601](http://arxiv.org/abs/2503.23601)|null|\n", "2503.22459": "|**2025-03-28**|**Control of Humanoid Robots with Parallel Mechanisms using Kinematic Actuation Models**|Nicolas Mansard Team|[2503.22459](http://arxiv.org/abs/2503.22459)|null|\n", "2503.22249": "|**2025-03-28**|**FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation**|Debin Zhao Team|[2503.22249](http://arxiv.org/abs/2503.22249)|null|\n", "2503.21257": "|**2025-03-27**|**OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation**|Wanting Li Team|[2503.21257](http://arxiv.org/abs/2503.21257)|null|\n", "2503.20842": "|**2025-03-26**|**Anti Robot Speciesism**|Miklos Sarvary Team|[2503.20842](http://arxiv.org/abs/2503.20842)|null|\n", "2503.19356": "|**2025-03-25**|**Can Vision-Language Models Answer Face to Face Questions in the Real-World?**|Roland Memisevic Team|[2503.19356](http://arxiv.org/abs/2503.19356)|null|\n", "2503.15082": "|**2025-03-19**|**StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion**|Siyuan Huang Team|[2503.15082](http://arxiv.org/abs/2503.15082)|null|\n", "2503.14734": "|**2025-03-27**|**GR00T N1: An Open Foundation Model for Generalist Humanoid Robots**|Yuke Zhu Team|[2503.14734](http://arxiv.org/abs/2503.14734)|null|\n", "2503.13441": "|**2025-03-24**|**Humanoid Policy ~ Human Policy**|Xiaolong Wang Team|[2503.13441](http://arxiv.org/abs/2503.13441)|null|\n", "2503.12725": "|**2025-03-17**|**Humanoids in Hospitals: A Technical Study of Humanoid Surrogates for Dexterous Medical Interventions**|Michael Yip Team|[2503.12725](http://arxiv.org/abs/2503.12725)|null|\n", "2503.12533": "|**2025-03-16**|**Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills**|Zongqing Lu Team|[2503.12533](http://arxiv.org/abs/2503.12533)|null|\n", "2503.11020": "|**2025-03-14**|**Fast and Robust Localization for Humanoid Soccer Robot via Iterative Landmark Matching**|Dennis W. Hong Team|[2503.11020](http://arxiv.org/abs/2503.11020)|null|\n", "2503.10626": "|**2025-03-13**|**NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models**|Michael Black Team|[2503.10626](http://arxiv.org/abs/2503.10626)|null|\n", "2503.10554": "|**2025-03-13**|**NuExo: A Wearable Exoskeleton Covering all Upper Limb ROM for Outdoor Data Collection and Teleoperation of Humanoid Robots**|Huimin Lu Team|[2503.10554](http://arxiv.org/abs/2503.10554)|null|\n", "2503.09015": "|**2025-03-12**|**Natural Humanoid Robot Locomotion with Generative Motion Prior**|Rong Xiong Team|[2503.09015](http://arxiv.org/abs/2503.09015)|null|\n", "2503.09010": "|**2025-03-13**|**HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception for Humanoid Robots**|Renjing Xu Team|[2503.09010](http://arxiv.org/abs/2503.09010)|null|\n", "2503.08349": "|**2025-03-11**|**LiPS: Large-Scale Humanoid Robot Reinforcement Learning with Parallel-Series Structures**|Renjing Xu Team|[2503.08349](http://arxiv.org/abs/2503.08349)|null|\n"}, "Dexterous": {"2503.23120": "|**2025-03-29**|**Dexterous Non-Prehensile Manipulation for Ungraspable Object via Extrinsic Dexterity**|Yuanpei Chen Team|[2503.23120](http://arxiv.org/abs/2503.23120)|null|\n", "2503.21860": "|**2025-03-27**|**ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning**|Siyuan Huang Team|[2503.21860](http://arxiv.org/abs/2503.21860)|null|\n", "2503.19457": "|**2025-03-25**|**G-DexGrasp: Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior Retrieval and Prior-Assisted Generation**|Ruizhen Hu Team|[2503.19457](http://arxiv.org/abs/2503.19457)|null|\n", "2503.12533": "|**2025-03-16**|**Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills**|Zongqing Lu Team|[2503.12533](http://arxiv.org/abs/2503.12533)|null|\n", "2503.10966": "|**2025-03-14**|**Is Your Imitation Learning Policy Better than Mine? Policy Comparison with Near-Optimal Stopping**|Haruki Nishimura Team|[2503.10966](http://arxiv.org/abs/2503.10966)|null|\n", "2503.09078": "|**2025-03-12**|**Sequential Multi-Object Grasping with One Dexterous Hand**|Daniel Seita Team|[2503.09078](http://arxiv.org/abs/2503.09078)|null|\n", "2503.08257": "|**2025-03-16**|**DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness**|Yuexin Ma Team|[2503.08257](http://arxiv.org/abs/2503.08257)|**[link](https://github.com/4DVLab/DexGrasp-Anything)**|\n", "2503.06669": "|**2025-03-13**|**AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems**|Jianchao Zhu Team|[2503.06669](http://arxiv.org/abs/2503.06669)|**[link](https://github.com/opendrivelab/agibot-world)**|\n", "2503.05995": "|**2025-03-08**|**ReJSHand: Efficient Real-Time Hand Pose Estimation and Mesh Reconstruction Using Refined Joint and Skeleton Features**|Hong Zhang Team|[2503.05995](http://arxiv.org/abs/2503.05995)|**[link](https://github.com/daishipeng/rejshand)**|\n", "2503.05231": "|**2025-03-07**|**Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction**|Bin He Team|[2503.05231](http://arxiv.org/abs/2503.05231)|null|\n", "2503.04014": "|**2025-03-06**|**Dexterous Hand Manipulation via Efficient Imitation-Bootstrapped Online Reinforcement Learning**|Xiaodong He Team|[2503.04014](http://arxiv.org/abs/2503.04014)|null|\n", "2503.03890": "|**2025-03-05**|**LensDFF: Language-enhanced Sparse Feature Distillation for Efficient Few-Shot Dexterous Manipulation**|Alois Knoll Team|[2503.03890](http://arxiv.org/abs/2503.03890)|null|\n", "2503.03102": "|**2025-03-05**|**Selective Tweezing and Immobilization of Colloids for Dexterous Manipulation of Biological Materials**|Kimani C. Toussaint Jr Team|[2503.03102](http://arxiv.org/abs/2503.03102)|null|\n", "2503.01789": "|**2025-03-03**|**TacCap: A Wearable FBG-Based Tactile Sensor for Seamless Human-to-Robot Skill Transfer**|Mark R. Cutkosky Team|[2503.01789](http://arxiv.org/abs/2503.01789)|null|\n", "2503.01616": "|**2025-03-03**|**RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation**|Jun Ma Team|[2503.01616](http://arxiv.org/abs/2503.01616)|null|\n", "2503.01543": "|**2025-03-03**|**Exo-ViHa: A Cross-Platform Exoskeleton System with Visual and Haptic Feedback for Efficient Dexterous Skill Learning**|Wenbo Ding Team|[2503.01543](http://arxiv.org/abs/2503.01543)|null|\n", "2503.01078": "|**2025-03-03**|**KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands**|Jeffrey Ichnowski Team|[2503.01078](http://arxiv.org/abs/2503.01078)|null|\n", "2502.20396": "|**2025-02-27**|**Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids**|Yuke Zhu Team|[2502.20396](http://arxiv.org/abs/2502.20396)|null|\n", "2502.19250": "|**2025-02-28**|**ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration**|Feifei Feng Team|[2502.19250](http://arxiv.org/abs/2502.19250)|null|\n", "2502.18423": "|**2025-02-26**|**Retrieval Dexterity: Efficient Object Retrieval in Clutters with Dexterous Hand**|Yuanpei Chen Team|[2502.18423](http://arxiv.org/abs/2502.18423)|null|\n"}}